{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging with maximum entropy models (10 pts)\n",
    "\n",
    "In this task you will build a maximum entropy model for part-of-speech tagging. As the name suggests, our problem is all about converting a sequence of words into a sequence of part-of-speech tags. \n",
    "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
    "\n",
    "\n",
    "__Your man goal:__ implement the model from [the article you're given](W96-0213.pdf).\n",
    "\n",
    "Unlike previous tasks, this one gives you greater degree of freedom and less automated tests. We provide you with programming interface but nothing more.\n",
    "\n",
    "__A piece of advice:__ there's a lot of objects happening here. If you don't understand why some object is needed, find `def train` function and see how everything is linked together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types:\n",
    "# Word: str\n",
    "# Sentence: list of str\n",
    "TaggedWord = collections.namedtuple('TaggedWord', ['text', 'tag'])\n",
    "# TaggedSentence: list of TaggedWord\n",
    "# Tags: list of TaggedWord\n",
    "# TagLattice: list of Tags\n",
    "\n",
    "def read_tagged_sentences(path):\n",
    "    \"\"\"\n",
    "    Read tagged sentences from CoNLL-U file and return array of TaggedSentence (array of lists of TaggedWord).\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "\n",
    "def write_tagged_sentence(tagged_sentence, f):\n",
    "    \"\"\"\n",
    "    Write tagged sentence in CoNLL-U format to file-like object f.\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "\n",
    "def read_tags(path):\n",
    "    \"\"\"\n",
    "    Read a list of possible tags from file and return the list.\n",
    "    \"\"\"\n",
    "    <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: evaluation\n",
    "\n",
    "We want you to estimate tagging quality by a simple accuracy: a fraction of tag predictions that turned out to be correct - averaged over the entire training corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types:\n",
    "TaggingQuality = collections.namedtuple('TaggingQuality', ['acc'])\n",
    "\n",
    "def tagging_quality(ref, out):\n",
    "    \"\"\"\n",
    "    Compute tagging quality and reutrn TaggingQuality object.\n",
    "    \"\"\"\n",
    "    nwords = 0\n",
    "    ncorrect = 0\n",
    "    import itertools\n",
    "    for ref_sentence, out_sentence in itertools.zip_longest(ref, out):\n",
    "        for ref_word, out_word in itertools.zip_longest():\n",
    "            ...\n",
    "    return ncorrect / nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Value and Update\n",
    "\n",
    "In order to implement two interlinked data structures: \n",
    "* __Value__ - a class that holds POS tagger's parameters. Basically an array of numbers\n",
    "* __Update__ - a class that stores updates for Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Dense object that holds parameters.\n",
    "        :param n: array length\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def dot(self, update):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def assign(self, other):\n",
    "        \"\"\"\n",
    "        self = other\n",
    "        other is Value.\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def assign_madd(self, x, coeff):\n",
    "        \"\"\"\n",
    "        self = self + x * coeff\n",
    "        x can be either Value or Update.\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "\n",
    "class Update:\n",
    "    \"\"\"\n",
    "    Sparse object that holds an update of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positions=None, values=None):\n",
    "        \"\"\"\n",
    "        positions: array of int\n",
    "        values: array of float\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def assign_madd(self, update, coeff):\n",
    "        \"\"\"\n",
    "        self = self + update * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Maximum Entropy POS Tagger\n",
    "_step 1 - draw an oval; step 2 - draw the rest of the owl (c)_\n",
    "\n",
    "In this secion you will implement a simple linear model to predict POS tags.\n",
    "Make sure you [read the article](W96-0213.pdf) before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types:\n",
    "Features = Update\n",
    "Hypo = collections.namedtuple('Hypo', ['prev', 'pos', 'tagged_word', 'score'])\n",
    "# prev: previous Hypo\n",
    "# pos: position of word (0-based)\n",
    "# tagged_word: tagging of source_sentence[pos]\n",
    "# score: sum of scores over edges\n",
    "\n",
    "TaggerParams = collections.namedtuple('FeatureParams', [\n",
    "    'src_window',\n",
    "    'dst_order',\n",
    "    'max_suffix',\n",
    "    'beam_size',\n",
    "    'nparams'\n",
    "    ])\n",
    "\n",
    "import cityhash\n",
    "def h(x):\n",
    "    \"\"\"\n",
    "    Compute CityHash of any object.\n",
    "    Can be used to construct features.\n",
    "    \"\"\"\n",
    "    return cityhash.CityHash64(repr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A thing that computes score and gradient for given features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._params = Value(n)\n",
    "\n",
    "    def params(self):\n",
    "        return self._params\n",
    "\n",
    "    def score(self, features):\n",
    "        \"\"\"\n",
    "        features: Update\n",
    "        \"\"\"\n",
    "        return self._params.dot(features)\n",
    "\n",
    "    def gradient(self, features, score):\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureComputer:\n",
    "    def __init__(self, tagger_params, source_sentence):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def compute_features(self, hypo):\n",
    "        \"\"\"\n",
    "        Compute features for a given Hypo and return Update.\n",
    "        \"\"\"\n",
    "        <YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V: Beam search\n",
    "\n",
    "We can find the most likely tagging approximately using Beam Search. As everything else, it comes with a separate interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchTask:\n",
    "    \"\"\"\n",
    "    An abstract beam search task. Can be used with beam_search() generic \n",
    "    function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tagger_params, source_sentence, model, tags):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def total_num_steps(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses between beginning and end (number of words in\n",
    "        the sentence).\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def beam_size(self):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def expand(self, hypo):\n",
    "        \"\"\"\n",
    "        Given Hypo, return a list of its possible expansions.\n",
    "        'hypo' might be None -- return a list of initial hypos then.\n",
    "\n",
    "        Compute hypotheses' scores inside this function!\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def recombo_hash(self, hypo):\n",
    "        \"\"\"\n",
    "        If two hypos have the same recombination hashes, they can be collapsed\n",
    "        together, leaving only the hypothesis with a better score.\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "\n",
    "\n",
    "def beam_search(beam_search_task):\n",
    "    \"\"\"\n",
    "    Return list of stacks.\n",
    "    Each stack contains several hypos, sorted by score in descending \n",
    "    order (i.e. better hypos first).\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentences(dataset, <YOUR_PARAMS>):\n",
    "    \"\"\"\n",
    "    Main predict function.\n",
    "    Tags all sentences in dataset. Dataset is a list of TaggedSentence; while \n",
    "    tagging, ignore existing tags.\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VI: Optimization objective and algorithm\n",
    "\n",
    "Once we defined our model and inference algorithm, we can define an optimization task: an object that computes loss function and its gradients w.r.t. model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTask:\n",
    "    \"\"\"\n",
    "    Optimization task that can be used with sgd().\n",
    "    \"\"\"\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Parameters which are optimized in this optimization task.\n",
    "        Return Value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        \"\"\"\n",
    "        Return (loss, gradient) on a specific example.\n",
    "\n",
    "        loss: float\n",
    "        gradient: Update\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class UnstructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, ...):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def params(self):\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        <YOUR CODE>\n",
    "\n",
    "\n",
    "class StructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, tagger_params, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.model = LinearModel(...)\n",
    "        self.tags = tags\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.params()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        # Do beam search.\n",
    "        beam_search_task = BeamSearchTask(\n",
    "            self.tagger_params, \n",
    "            [golden_tagged_word.text for golden_tagged_word in golden_sentence], \n",
    "            self.model, \n",
    "            self.tags\n",
    "            )\n",
    "        stacks = beam_search(beam_search_task)\n",
    "\n",
    "        # Compute chain of golden hypos (and their scores!).\n",
    "        golden_hypo = None\n",
    "        feature_computer = ...\n",
    "        for i in range(len(golden_sentence)):\n",
    "            new_golden_hypo = ...\n",
    "            golden_hypo = golden_hypo\n",
    "\n",
    "        # Find where to update.\n",
    "        golden_head = <YOUR CODE>\n",
    "        rival_head = <YOUR CODE>\n",
    "\n",
    "        # Compute gradient.\n",
    "        grad = Update()\n",
    "        while golden_head and rival_head:\n",
    "            rival_features = feature_computer.compute_features(rival_head)\n",
    "            grad.assign_madd(self.model.gradient(rival_features, score=None), 1)\n",
    "\n",
    "            golden_features = feature_computer.compute_features(golden_head)\n",
    "            grad.assign_madd(self.model.gradient(golden_features, score=None), -1)\n",
    "\n",
    "\n",
    "            golden_head = golden_head.prev\n",
    "            rival_head = rival_head.prev\n",
    "\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VII: optimizer\n",
    "\n",
    "By this point we can define a model with parameters $\\theta$ and a problem that computes gradients $ \\partial L \\over \\partial \\theta $ w.r.t. model parameters.\n",
    "\n",
    "Optimization is performed by gradient descent: $ \\theta := \\theta - \\alpha {\\partial L \\over \\partial \\theta} $\n",
    "\n",
    "In order to speed up training, we use stochastic gradient descent that operates on minibatches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDParams = collections.namedtuple('SGDParams', [\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'minibatch_size',\n",
    "    'average' # bool or int\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_batches(dataset, minibatch_size):\n",
    "    \"\"\"\n",
    "    Make list of batches from a list of examples.\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "\n",
    "\n",
    "def sgd(sgd_params, optimization_task, dataset, after_each_epoch_fn):\n",
    "    \"\"\"\n",
    "    Run (averaged) SGD on a generic optimization task. Modify optimization\n",
    "    task's parameters.\n",
    "\n",
    "    After each epoch (and also before and after the whole training),\n",
    "    run after_each_epoch_fn().\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VIII: Training loop\n",
    "\n",
    "The train function combines everthing you used below to get new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    tags='./data/tags',\n",
    "    train_dataset='./data/en-ud-train.conllu',\n",
    "    dev_dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    sgd_epochs=15,\n",
    "    sgd_learning_rate=0.01,\n",
    "    sgd_minibatch_size=32,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_src_window=2,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_dst_order=3,\n",
    "    \n",
    "    # Maximal number of prefix/suffix letters to use for features\n",
    "    tagger_max_suffix=4,\n",
    "    \n",
    "    # Width for beam search (0 means unstructured)\n",
    "    beam_size=1,\n",
    "    \n",
    "    # Parameter vector size (for hashing)\n",
    "    nparams= 2 * 22,\n",
    "):\n",
    "    \"\"\" Train a pos-tagger model and save it's parameters to :model: \"\"\"\n",
    "\n",
    "    # Beam size.\n",
    "    optimization_task_cls = StructuredPerceptronOptimizationTask\n",
    "    if beam_size == 0:\n",
    "        beam_size = 1\n",
    "        optimization_task_cls = UnstructuredPerceptronOptimizationTask\n",
    "\n",
    "    # Parse cmdargs.\n",
    "    tags = read_tags(cmdargs.tags)\n",
    "    train_dataset = read_tagged_sentences(train_dataset)\n",
    "    dev_dataset = read_tagged_sentences(dev_dataset)\n",
    "    params = None\n",
    "    if os.path.exists(cmdargs.model):\n",
    "        params = pickle.load(open(cmdargs.model, 'rb'))\n",
    "    sgd_params = SGDParams(\n",
    "        epochs=sgd_epochs,\n",
    "        learning_rate=sgd_learning_rate,\n",
    "        minibatch_size=sgd_minibatch_size,\n",
    "        average=sgd_average\n",
    "        )\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=nparams\n",
    "        )\n",
    "\n",
    "    # Load optimization task\n",
    "    optimization_task = optimization_task_cls(...)\n",
    "    if params is not None:\n",
    "        print('\\n\\nLoading parameters from %s\\n\\n' % cmdargs.model)\n",
    "        optimization_task.params().assign(params)\n",
    "\n",
    "    # Validation.\n",
    "    def after_each_epoch_fn():\n",
    "        model = LinearModel(cmdargs.nparams)\n",
    "        model.params().assign(optimization_task.params())\n",
    "        tagged_sentences = tag_sentences(dev_dataset, <YOUR_PARAMS>)\n",
    "        q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dev_dataset))\n",
    "        print()\n",
    "        print(q)\n",
    "        print()\n",
    "\n",
    "        # Save parameters.\n",
    "        print('\\n\\nSaving parameters to %s\\n\\n' % cmdargs.model)\n",
    "        pickle.dump(optimization_task.params(), open(cmdargs.model, 'wb'))\n",
    "\n",
    "    # Run SGD.\n",
    "    sgd(sgd_params, optimization_task, train_dataset, after_each_epoch_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model with default params\n",
    "train(model='./default_model.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IX: Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-c5d1753b0c3f>, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-c5d1753b0c3f>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    tagged_sentences = tag_sentences(dataset, <YOUR_PARAMS>)\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def test(\n",
    "    tags='./data/tags',\n",
    "    dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    # model and inference params; see train for their description\n",
    "    tagger_src_window=2,\n",
    "    tagger_dst_order=3,\n",
    "    tagger_max_suffix=4,\n",
    "    beam_size=1,\n",
    "):\n",
    "\n",
    "\n",
    "    tags = read_tags(tags)\n",
    "    dataset = read_tagged_sentences(dataset)\n",
    "    params = pickle.load(open(model, 'rb'))\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=0\n",
    "        )\n",
    "\n",
    "    # Load model.\n",
    "    model = LinearModel(params.values.shape[0])\n",
    "    model.params().assign(params)\n",
    "\n",
    "    # Tag all sentences.\n",
    "    tagged_sentences = tag_sentences(dataset, <YOUR_PARAMS>)\n",
    "\n",
    "    # Write tagged sentences.\n",
    "    for tagged_sentence in tagged_sentences:\n",
    "        write_tagged_sentence(tagged_sentence, sys.stdout)\n",
    "\n",
    "    # Measure and print quality.\n",
    "    q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dataset))\n",
    "    print(q, file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "test(model='./default_model.npz')\n",
    "\n",
    "# sanity chec: accuracy > 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part X: play with it\n",
    "\n",
    "_This part is optional_\n",
    "\n",
    "Once you've built something, it's only natural to test the limits of your contraption.\n",
    "\n",
    "At minumum, we want you to find out how default model accuracy depends on __beam size__\n",
    "\n",
    "To get maximum points, your model should get final quality >= 93% \n",
    "\n",
    "Any further analysis is welcome, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
