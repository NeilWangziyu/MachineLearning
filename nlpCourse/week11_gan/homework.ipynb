{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embedding_mapping.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/embedding_mapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework: Un(semi)-supervised word translation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework based on [Conneau et al. 2018](https://arxiv.org/abs/1710.04087) article.\n",
    "\n",
    "In the homework we offer you to train a mapping between Ukrainian word vectors and Russian word vectors just like in the first homework of the NLP course. But unlike the first homework this mapping will be build (almost) unsupervised: without parallel data (pairs of corresponding words in Ukrainian and Russian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env KERAS_BACKEND=tensorflow\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers as L\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from IPython import display\n",
    "from tqdm import tnrange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/cnwyfbfa44mqxph/ukr_rus.train.txt?dl=1 -O ./ukr_rus.train.txt\n",
    "!wget https://www.dropbox.com/s/78otz1d4d9b0284/ukr_rus.test.txt?dl=1 -O ./ukr_rus.test.txt\n",
    "!wget https://www.dropbox.com/s/210m7gwqkikpsxd/uk.w2v.bin?dl=1 -O ./uk.w2v.bin\n",
    "!wget https://www.dropbox.com/s/3luwyjdmofsdfjz/ru.w2v.bin?dl=1 -O ./ru.w2v.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_embs = gensim.models.KeyedVectors.load_word2vec_format(\"ru.w2v.bin\", binary=True)\n",
    "uk_embs = gensim.models.KeyedVectors.load_word2vec_format(\"uk.w2v.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = uk_embs.vectors[:50000]\n",
    "y = ru_embs.vectors[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pairs, uk_vectors, topn=1):\n",
    "    \"\"\" TODO maybe insert docstring \"\"\"\n",
    "    assert len(pairs) == len(uk_vectors)\n",
    "    num_matches = 0\n",
    "    for i, (uk, ru) in enumerate(pairs):\n",
    "        num_matches += ru in set(w[0] for w in ru_embs.most_similar([uk_vectors[i]], topn=topn))\n",
    "    return num_matches / len(pairs)\n",
    "\n",
    "def load_word_pairs(filename):\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    with open(filename, \"r\") as inpf:\n",
    "        for line in inpf:\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if uk not in uk_embs or ru not in ru_embs:\n",
    "                continue\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_embs[uk])\n",
    "            ru_vectors.append(ru_embs[ru])\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_ru_test, x_test, y_test = load_word_pairs(\"ukr_rus.test.txt\")\n",
    "uk_ru_train, x_train, y_train = load_word_pairs(\"ukr_rus.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(uk_ru_test, x_test, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding space mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called Procrustes problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$$\n",
    "or\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F$$\n",
    "\n",
    "where $||*||_F$ - Frobenius norm.\n",
    "\n",
    "In Greek mythology, Procrustes or \"the stretcher\" was a rogue smith and bandit from Attica who attacked people by stretching them or cutting off their legs, so as to force them to fit the size of an iron bed. We make same bad things with source embedding space. Our Procrustean bed is target embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait...$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$ looks like simple multiple linear regression (without intercept fit). So let's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Procrustean Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown (see original paper) that a self-consistent linear mapping between semantic spaces should be orthogonal. TODO simplify phrases\n",
    "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, where: } W^TW = I$$\n",
    "\n",
    "$$I \\text{- identity matrix}$$\n",
    "\n",
    "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "$$W^*=UV^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word translation learning using GAN (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\mathcal{X}=\\{x_1,...,x_n\\} \\subset \\mathrm{R}^d$ - source embedding set, and $\\mathcal{Y}=\\{y_1,...,y_m\\} \\subset \\mathrm{R}^d$ - target embedding set, then discriminator is simply orthogonal mapping that can be defined as square matrix: $W\\in O_d(\\mathrm{R})$.\n",
    "\n",
    "In terms of neural network, generator is a network with single linear layer with orthogonality constraint and without nonlinearity after it.\n",
    "\n",
    "The generator input is a source embedding $x_i$, the generator output is a mapped source embedding $Wx_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras, keras.layers as L\n",
    "\n",
    "def build_generator(emb_size):\n",
    "    # TIPS: use keras.Sequential and keras.initializers\n",
    "    # YOUR_CODE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator(EMB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator is a neural network that should discriminate between objects from $W\\mathcal{X}$ (mapped source embeddings) and objects from $\\mathcal{Y}$ (target embeddings).\n",
    "\n",
    "Just like in original article for discriminator we will use a multilayer perceptron with two hidden layers of size 2048, and Leaky-ReLU activation functions. The input to the discriminator is corrupted with dropout noise\n",
    "with a rate of 0.1.\n",
    "\n",
    "The discriminator input is either mapped source embedding $Wx_i$ or target embedding $y_j$, the discriminator output is a probability of input to be from source distribution $p_D=p_D(source=1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(emb_size):\n",
    "    # YOUR_CODE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(EMB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of discriminator is to maximize output probability for mapped source embeddings $p_D(source=1|Wx_i)$ and minimize probability for target embeddings $p_D(source=1|y_j)$. The last is equivalent to maximization of  $p_D(source=0|y_j)$. Thus, we can train this classifier with standard cross-entropy loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L}_D(\\theta_D|W)=-\\frac{1}{n}\\sum_{i=1}^n\\log p_D(source=1|Wx_i)-\\frac{1}{m}\\sum_{i=1}^m\\log p_D(source=0|y_i)$$\n",
    "Equivalent:\n",
    "$$\\mathcal{L}_D(\\theta_D|W)=-\\frac{1}{n}\\sum_{i=1}^n\\log p_D(source=1|Wx_i)-\\frac{1}{m}\\sum_{i=1}^m\\log (1-p_D(source=1|y_i))$$\n",
    "\n",
    "**NB:** We minimize $\\mathcal{L}_D(\\theta_D|W)$ with respect discriminator parameters $\\theta_D$. The matrix $W$ is fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR_CODE HERE\n",
    "\n",
    "#X = \n",
    "#Y = \n",
    "#W = \n",
    "#WX =\n",
    "\n",
    "#logp_wx_is_real = \n",
    "#logp_wx_is_fake = \n",
    "#logp_y_is_real = \n",
    "\n",
    "# L_d = L_d_source + L_d_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suggested Goodfellow (2016) it is useful to use soft targets instead hard ones. In case label smoothing:\n",
    "$$\\mathcal{L}_D(\\theta_D|W)=\\mathcal{L}_{D_1}+\\mathcal{L}_{D_2}$$\n",
    "\n",
    "Where:\n",
    "$$\\mathcal{L}_{D_1}=\\frac{1}{n}\\sum_{i=1}^n[(1-\\alpha)\\log p_D(source=1|Wx_i) + \\alpha\\log p_D(source=0|Wx_i)]$$\n",
    "\n",
    "$$\\mathcal{L}_{D_2}=\\frac{1}{m}\\sum_{i=1}^n[(1-\\alpha)\\log p_D(source=0|Wx_i) + \\alpha\\log p_D(source=1|Wx_i)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE IF YOU REALLY WANT TO USE LABEL SMOOTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of generator is to fool discriminator, i.e. to produce mapping $W\\mathcal{X}$ indistinguishable from $\\mathcal{Y}$. Therefore we should turn over discriminator loss, minimize output probability for mapped source embeddings $p_D(source=1|Wx_i)$ and minimize probability for target embeddings $p_D(source=0|y_j)$.\n",
    "\n",
    "$$\\mathcal{L}_G(W|\\theta_D)=-\\frac{1}{n}\\sum_{i=1}^n\\log (1-p_D(source=1|Wx_i))-\\frac{1}{m}\\sum_{i=1}^m\\log p_D(source=1|y_i)$$\n",
    "\n",
    "**NB:** We minimize $\\mathcal{L}_G(W|\\theta_D)$ with respect matrix $W$ coefficients. Disciminator parameters $\\theta_D$ is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because gradients do not flow through generator for target samples:\n",
    "\n",
    "$$\\mathcal{L}_G(W|\\theta_D)=-\\frac{1}{n}\\sum_{i=1}^n\\log (1-p_D(source=1|Wx_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast with original article to be more stable we allow you to add a supervised component of loss - MSE for small number of fixed pairs vectors from $\\mathcal{X}$ and $\\mathcal{Y}$.\n",
    "\n",
    "$$\\mathcal{L}_G(W|\\theta_D)=-\\frac{1}{n}\\sum_{i=1}^n\\log (1-p_D(source=1|Wx_i))+\\gamma \\frac{1}{N}\\sum_{k}^N(Wx_k-y_k)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pair = tf.placeholder('float32', [None, EMB_SIZE])\n",
    "#Y_pair = tf.placeholder('float32', [None, EMB_SIZE])\n",
    "\n",
    "# YOUR_CODE_HERE\n",
    "\n",
    "#L_g = L_mse * 100 + L_g_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonality constraint\n",
    "Conneau et al. propose to use a simple update step to ensure that the matrix $W$ stays close to an\n",
    "orthogonal matrix during training:\n",
    "\n",
    "$$W \\gets (1+\\beta)W-\\beta(WW^T)W$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = tf.constant(0.01)\n",
    "\n",
    "# TIPS: USE tf.assing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING_RATE = 0.1\n",
    "# GRADIENT DESCENT OPTIMIZER?\n",
    "\n",
    "#gen_optim =\n",
    "#dis_optim ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def sample_batch(bsize):\n",
    "    x_batch = x[np.random.choice(np.arange(x.shape[0]), size=bsize)]\n",
    "    y_batch = y[np.random.choice(np.arange(y.shape[0]), size=bsize)]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step():\n",
    "    # YOUR_CODE\n",
    "    \n",
    "def generator_step():\n",
    "    # YOUR_CODE\n",
    "\n",
    "def orthogonolize_step():\n",
    "    sess.run(orthogonolize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics():\n",
    "    feed_dict = {\n",
    "        X: x_test,\n",
    "        Y: y_test,\n",
    "        X_pair: x_train[:50],\n",
    "        Y_pair: y_train[:50]\n",
    "    }\n",
    "    loss_g, loss_d, logp_x, logp_y, wx = sess.run([L_g, L_d, logp_wx_is_real, logp_y_is_real, WX], feed_dict)\n",
    "    return loss_g, loss_d, np.exp(logp_x), np.exp(logp_y), wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = keras.backend.get_session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "EPOCH_SIZE = 1000\n",
    "DIS_STEPS = 5\n",
    "GEN_STEPS = 1\n",
    "\n",
    "\n",
    "gen_loss_history = []\n",
    "dis_loss_history = []\n",
    "prec_history = []\n",
    "\n",
    "for epoch_num in range(N_EPOCHS):\n",
    "    print(\"Epoch: {}\".format(epoch_num + 1))\n",
    "    for batch_num in tnrange(EPOCH_SIZE):\n",
    "        \n",
    "        for _ in range(DIS_STEPS):\n",
    "            # YOUR_CODE\n",
    "            \n",
    "        for _ in range(GEN_STEPS):\n",
    "            # YOUR_CODE\n",
    "        \n",
    "        if batch_num % 10 == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            loss_g, loss_d, p_x, p_y, wx = get_metrics()\n",
    "            gen_loss_history.append(loss_g)\n",
    "            dis_loss_history.append(loss_d)\n",
    "            \n",
    "            if batch_num % 100 == 0:\n",
    "                prec_history.append(precision(uk_ru_test, wx, 5))\n",
    "\n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.subplot(212)\n",
    "            plt.plot(gen_loss_history, label=\"Generator loss\")\n",
    "            plt.plot(dis_loss_history, label=\"Discriminator loss\")\n",
    "            plt.legend(loc='best')\n",
    "\n",
    "            plt.subplot(221)\n",
    "            plt.title('Mapped vs target data')\n",
    "            plt.hist(p_x, label='D(Y)', alpha=0.5,range=[0,1], bins=20)\n",
    "            plt.hist(p_y, label='D(WX)',alpha=0.5,range=[0,1], bins=20)\n",
    "            plt.legend(loc='best')\n",
    "            \n",
    "            plt.subplot(222)\n",
    "            plt.title('Precision top5')\n",
    "            plt.plot(prec_history)\n",
    "            plt.show()\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully unsupervised word translation learning (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to exclude MSE term from generator loss and train GAN with sufficient quality (~40% precision top5). You should tune parameters of optimizers and training schedule to make it stable.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goto-env",
   "language": "python",
   "name": "goto-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
