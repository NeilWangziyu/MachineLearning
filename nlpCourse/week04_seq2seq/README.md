### Materials
* __Lecture [slides](https://github.com/yandexdataschool/nlp_course/raw/master/resources/slides/nlp18_04_seq2seq_attention.pdf)__
* Our videos (russian): [lecture](https://yadi.sk/i/3y9JX6Q_0ZLqpA), [seminar](https://yadi.sk/i/cB5606vgbOlBog)
* Stanford lecture on seq2seq and MT (english) - [video](https://www.youtube.com/watch?v=IxQtK2SjWWM)
* Alternative CMU lectures - [seq2seq](https://www.youtube.com/watch?v=aHkgjfKvIhk&list=PL8PYTP1V4I8Ba7-rY4FoB4-jfuJ7VDKEE&index=20) and [attention](https://www.youtube.com/watch?v=ullLRKZ99qQ&index=21&list=PL8PYTP1V4I8Ba7-rY4FoB4-jfuJ7VDKEE)

### Practice
This time we're gonna use a shared `practice.ipynb` for both seminar and homework.

Seminar [colab url](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/master/week04_seq2seq/practice.ipynb).

#### RNN Encoder-Decoder (Vanilla):
![vanilla_enc_dec](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/vanilla_enc_dec.gif)

#### Attention mechanism:
![attention_mechanism](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/attention_mechanism.gif)

### More on RNN
* Distill.pub post on attention and augmentations for RNN - [post](https://distill.pub/2016/augmented-rnns/)
* Seq2seq lecture - [video](https://www.youtube.com/watch?v=G5RY_SUJih4)
* [BLEU](http://www.aclweb.org/anthology/P02-1040.pdf) and [CIDEr](https://arxiv.org/pdf/1411.5726.pdf) articles.
* Image captioning
  * MSCOCO captioning [challenge](http://mscoco.org/dataset/#captions-challenge2015)
  * Captioning baseline [notebook](https://github.com/yandexdataschool/Practical_DL/tree/fall18/week07_seq2seq)
* Lecture on attention mechanisms - [video](https://www.youtube.com/watch?v=_XRBlhzb31U) (RUSSIAN)

### More on Transformer
* Illustrated transformer [post](https://jalammar.github.io/illustrated-transformer/)

