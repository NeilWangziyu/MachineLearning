{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorwatch as tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"326pt\" height=\"695pt\"\n",
       " viewBox=\"0.00 0.00 325.77 694.92\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(72 622.9199)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-72,72 -72,-622.9199 253.7715,-622.9199 253.7715,72 -72,72\"/>\n",
       "<!-- Net/MaxPool2d[pool]/outputs/13 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Net/MaxPool2d[pool]/outputs/13</title>\n",
       "<g id=\"a_node1\"><a xlink:title=\"{&#39;kernel_shape&#39;: [2, 2], &#39;pads&#39;: [0, 0, 0, 0], &#39;strides&#39;: [2, 2]}\">\n",
       "<polygon fill=\"#8de5a1\" stroke=\"#7c96bc\" points=\"153.5,-466.7666 78.5,-466.7666 78.5,-430.7666 153.5,-430.7666 153.5,-466.7666\"/>\n",
       "<text text-anchor=\"start\" x=\"86.6733\" y=\"-446.7666\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">MaxPool2x2</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 14666907436370736755 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>14666907436370736755</title>\n",
       "<g id=\"a_node7\"><a xlink:title=\"{&#39;dilations&#39;: [1, 1], &#39;group&#39;: 1, &#39;kernel_shape&#39;: [5, 5], &#39;pads&#39;: [0, 0, 0, 0], &#39;strides&#39;: [1, 1]}\">\n",
       "<polygon fill=\"#a1c9f4\" stroke=\"#7c96bc\" points=\"164,-382.6133 68,-382.6133 68,-346.6133 164,-346.6133 164,-382.6133\"/>\n",
       "<text text-anchor=\"start\" x=\"75.6069\" y=\"-362.6133\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">Conv5x5 &gt; Relu</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Net/MaxPool2d[pool]/outputs/13&#45;&gt;14666907436370736755 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Net/MaxPool2d[pool]/outputs/13&#45;&gt;14666907436370736755</title>\n",
       "<path fill=\"none\" stroke=\"#7c96bc\" d=\"M116,-430.5066C116,-419.5893 116,-405.4824 116,-393.1239\"/>\n",
       "<polygon fill=\"#7c96bc\" stroke=\"#7c96bc\" points=\"119.5001,-392.8376 116,-382.8377 112.5001,-392.8377 119.5001,-392.8376\"/>\n",
       "<text text-anchor=\"middle\" x=\"145.707\" y=\"-404.7666\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\"> 1x6x14x14</text>\n",
       "</g>\n",
       "<!-- Net/MaxPool2d[pool]/outputs/16 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Net/MaxPool2d[pool]/outputs/16</title>\n",
       "<g id=\"a_node2\"><a xlink:title=\"{&#39;kernel_shape&#39;: [2, 2], &#39;pads&#39;: [0, 0, 0, 0], &#39;strides&#39;: [2, 2]}\">\n",
       "<polygon fill=\"#8de5a1\" stroke=\"#7c96bc\" points=\"153.5,-298.46 78.5,-298.46 78.5,-262.46 153.5,-262.46 153.5,-298.46\"/>\n",
       "<text text-anchor=\"start\" x=\"86.6733\" y=\"-278.46\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">MaxPool2x2</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Net/outputs/18 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Net/outputs/18</title>\n",
       "<polygon fill=\"#fffea3\" stroke=\"#7c96bc\" points=\"75,-214.3066 17,-214.3066 17,-178.3066 75,-178.3066 75,-214.3066\"/>\n",
       "<text text-anchor=\"start\" x=\"24.8013\" y=\"-194.3066\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">Reshape</text>\n",
       "</g>\n",
       "<!-- Net/MaxPool2d[pool]/outputs/16&#45;&gt;Net/outputs/18 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Net/MaxPool2d[pool]/outputs/16&#45;&gt;Net/outputs/18</title>\n",
       "<path fill=\"none\" stroke=\"#7c96bc\" d=\"M100.811,-262.1999C91.0877,-250.5107 78.3228,-235.1648 67.5582,-222.2237\"/>\n",
       "<polygon fill=\"#7c96bc\" stroke=\"#7c96bc\" points=\"70.2451,-219.9807 61.1593,-214.531 64.8635,-224.4572 70.2451,-219.9807\"/>\n",
       "<text text-anchor=\"middle\" x=\"111.5283\" y=\"-236.46\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\"> 1x16x5x5</text>\n",
       "</g>\n",
       "<!-- Net/outputs/17 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Net/outputs/17</title>\n",
       "<g id=\"a_node3\"><a xlink:title=\"{&#39;value&#39;: tensor([ &#45;1, 400])}\">\n",
       "<polygon fill=\"#ff9f9b\" stroke=\"#7c96bc\" points=\"60,-298.46 0,-298.46 0,-262.46 60,-262.46 60,-298.46\"/>\n",
       "<text text-anchor=\"start\" x=\"7.5977\" y=\"-278.46\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">Constant</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Net/outputs/17&#45;&gt;Net/outputs/18 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Net/outputs/17&#45;&gt;Net/outputs/18</title>\n",
       "<path fill=\"none\" stroke=\"#7c96bc\" d=\"M33.4718,-262.1999C35.5684,-251.1723 38.2838,-236.8904 40.6504,-224.4433\"/>\n",
       "<polygon fill=\"#7c96bc\" stroke=\"#7c96bc\" points=\"44.1055,-225.0088 42.535,-214.531 37.2287,-223.7013 44.1055,-225.0088\"/>\n",
       "</g>\n",
       "<!-- 13186416583165169327 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>13186416583165169327</title>\n",
       "<g id=\"a_node8\"><a xlink:title=\"{&#39;alpha&#39;: 1.0, &#39;beta&#39;: 1.0, &#39;transB&#39;: 1}\">\n",
       "<polygon fill=\"#8de5a1\" stroke=\"#7c96bc\" points=\"87.5,-130.1533 4.5,-130.1533 4.5,-84.1533 87.5,-84.1533 87.5,-130.1533\"/>\n",
       "<text text-anchor=\"start\" x=\"12.5889\" y=\"-114.1533\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">Linear &gt; Relu</text>\n",
       "<text text-anchor=\"start\" x=\"71.7246\" y=\"-92.1533\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">x2</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- Net/outputs/18&#45;&gt;13186416583165169327 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Net/outputs/18&#45;&gt;13186416583165169327</title>\n",
       "<path fill=\"none\" stroke=\"#7c96bc\" d=\"M46,-178.2651C46,-167.453 46,-153.3663 46,-140.5855\"/>\n",
       "<polygon fill=\"#7c96bc\" stroke=\"#7c96bc\" points=\"49.5001,-140.2574 46,-130.2575 42.5001,-140.2575 49.5001,-140.2574\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.4316\" y=\"-152.3066\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\"> 1x400</text>\n",
       "</g>\n",
       "<!-- Net/Linear[fc3]/outputs/23 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Net/Linear[fc3]/outputs/23</title>\n",
       "<g id=\"a_node5\"><a xlink:title=\"{&#39;alpha&#39;: 1.0, &#39;beta&#39;: 1.0, &#39;transB&#39;: 1}\">\n",
       "<polygon fill=\"#4878d0\" stroke=\"#7c96bc\" points=\"73,-36 19,-36 19,0 73,0 73,-36\"/>\n",
       "<text text-anchor=\"start\" x=\"30.5654\" y=\"-16\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">Linear</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 7523091584521392650 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>7523091584521392650</title>\n",
       "<g id=\"a_node6\"><a xlink:title=\"{&#39;dilations&#39;: [1, 1], &#39;group&#39;: 1, &#39;kernel_shape&#39;: [5, 5], &#39;pads&#39;: [0, 0, 0, 0], &#39;strides&#39;: [1, 1]}\">\n",
       "<polygon fill=\"#a1c9f4\" stroke=\"#7c96bc\" points=\"164,-550.9199 68,-550.9199 68,-514.9199 164,-514.9199 164,-550.9199\"/>\n",
       "<text text-anchor=\"start\" x=\"75.6069\" y=\"-530.9199\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\">Conv5x5 &gt; Relu</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- 7523091584521392650&#45;&gt;Net/MaxPool2d[pool]/outputs/13 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>7523091584521392650&#45;&gt;Net/MaxPool2d[pool]/outputs/13</title>\n",
       "<path fill=\"none\" stroke=\"#7c96bc\" d=\"M116,-514.6599C116,-503.7426 116,-489.6357 116,-477.2772\"/>\n",
       "<polygon fill=\"#7c96bc\" stroke=\"#7c96bc\" points=\"119.5001,-476.9909 116,-466.991 112.5001,-476.991 119.5001,-476.9909\"/>\n",
       "<text text-anchor=\"middle\" x=\"145.707\" y=\"-488.9199\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\"> 1x6x28x28</text>\n",
       "</g>\n",
       "<!-- 14666907436370736755&#45;&gt;Net/MaxPool2d[pool]/outputs/16 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>14666907436370736755&#45;&gt;Net/MaxPool2d[pool]/outputs/16</title>\n",
       "<path fill=\"none\" stroke=\"#7c96bc\" d=\"M116,-346.3533C116,-335.4359 116,-321.329 116,-308.9706\"/>\n",
       "<polygon fill=\"#7c96bc\" stroke=\"#7c96bc\" points=\"119.5001,-308.6843 116,-298.6843 112.5001,-308.6844 119.5001,-308.6843\"/>\n",
       "<text text-anchor=\"middle\" x=\"148.8857\" y=\"-320.6133\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\"> 1x16x10x10</text>\n",
       "</g>\n",
       "<!-- 13186416583165169327&#45;&gt;Net/Linear[fc3]/outputs/23 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>13186416583165169327&#45;&gt;Net/Linear[fc3]/outputs/23</title>\n",
       "<path fill=\"none\" stroke=\"#7c96bc\" d=\"M46,-83.7293C46,-72.2374 46,-58.3039 46,-46.2283\"/>\n",
       "<polygon fill=\"#7c96bc\" stroke=\"#7c96bc\" points=\"49.5001,-46.2027 46,-36.2028 42.5001,-46.2028 49.5001,-46.2027\"/>\n",
       "<text text-anchor=\"middle\" x=\"60.2529\" y=\"-58.1533\" font-family=\"Verdana\" font-size=\"10.00\" fill=\"#202020\"> 1x84</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<tensorwatch.model_graph.hiddenlayer.graph.Graph at 0x1266557b8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw.draw_model(net, [1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from scipy import interpolate\n",
    "from scipy import fftpack\n",
    "import scipy.stats as stats\n",
    "from scipy.fftpack import fft\n",
    "from torch.nn import functional\n",
    "from scipy.signal import butter, filtfilt\n",
    "import scipy.signal as signal\n",
    "import scipy.io as scio\n",
    "import torch\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "class ResidualBlock_1d(nn.Module):\n",
    "    '''\n",
    "    residual block small\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inchannel, outchannel, stride=1):\n",
    "        super(ResidualBlock_1d, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv1d(inchannel, outchannel, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm1d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(outchannel, outchannel, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm1d(outchannel)\n",
    "        )\n",
    "        if inchannel == outchannel:\n",
    "            self.right = None\n",
    "        else:\n",
    "            self.right = nn.Sequential(\n",
    "                nn.Conv1d(inchannel, outchannel, 1, stride, bias=False),\n",
    "                nn.BatchNorm1d(outchannel))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        if self.right is None:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = self.right(x)\n",
    "        out += residual\n",
    "        return functional.relu(out)\n",
    "\n",
    "\n",
    "class InceptionC_1d_improve(nn.Module):\n",
    "    def __init__(self, in_channels, channels_7x7=16):\n",
    "        super(InceptionC_1d_improve, self).__init__()\n",
    "        self.branch5_1 = nn.Sequential(nn.Conv1d(in_channels, 6, kernel_size=1),\n",
    "                                       nn.BatchNorm1d(6),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "        self.branch5_2 = nn.Sequential(nn.Conv1d(6, 6, kernel_size=5, padding=2, stride=2),\n",
    "                                       nn.BatchNorm1d(6),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "\n",
    "        self.branch3dbl_1 = nn.Sequential(nn.Conv1d(in_channels, 8, kernel_size=1),\n",
    "                                          nn.BatchNorm1d(8),\n",
    "                                          nn.ReLU(inplace=True))\n",
    "        self.branch3dbl_2 = nn.Sequential(nn.Conv1d(8, 12, kernel_size=3, padding=1),\n",
    "                                          nn.BatchNorm1d(12),\n",
    "                                          nn.ReLU(inplace=True))\n",
    "        self.branch3dbl_3 = nn.Sequential(nn.Conv1d(12, 12, kernel_size=3, padding=1, stride=2),\n",
    "                                          nn.BatchNorm1d(12),\n",
    "                                          nn.ReLU(inplace=True))\n",
    "\n",
    "        self.branch1x1 = nn.Sequential(nn.Conv1d(in_channels, 8, kernel_size=1, stride=2),\n",
    "                                       nn.BatchNorm1d(8),\n",
    "                                       nn.ReLU(inplace=True))\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = nn.Sequential(nn.Conv1d(in_channels, c7, kernel_size=1),\n",
    "                                         nn.BatchNorm1d(c7),\n",
    "                                         nn.ReLU(inplace=True))\n",
    "        self.branch7x7_2 = nn.Sequential(nn.Conv1d(c7, c7, kernel_size=7, padding=3),\n",
    "                                         nn.BatchNorm1d(c7),\n",
    "                                         nn.ReLU(inplace=True))\n",
    "        self.branch7x7_3 = nn.Sequential(nn.Conv1d(c7, 8, kernel_size=7, padding=3, stride=2),\n",
    "                                         nn.BatchNorm1d(8),\n",
    "                                         nn.ReLU(inplace=True))\n",
    "        self.branch7x7dbl_1 = nn.Sequential(nn.Conv1d(in_channels, c7, kernel_size=1),\n",
    "                                            nn.BatchNorm1d(c7),\n",
    "                                            nn.ReLU(inplace=True))\n",
    "        self.branch7x7dbl_2 = nn.Sequential(nn.Conv1d(c7, c7, kernel_size=7, padding=3),\n",
    "                                            nn.BatchNorm1d(c7),\n",
    "                                            nn.ReLU(inplace=True))\n",
    "        self.branch7x7dbl_3 = nn.Sequential(nn.Conv1d(c7, c7, kernel_size=7, padding=3),\n",
    "                                            nn.BatchNorm1d(c7),\n",
    "                                            nn.ReLU(inplace=True))\n",
    "        self.branch7x7dbl_4 = nn.Sequential(nn.Conv1d(c7, c7, kernel_size=7, padding=3),\n",
    "                                            nn.BatchNorm1d(c7),\n",
    "                                            nn.ReLU(inplace=True))\n",
    "        self.branch7x7dbl_5 = nn.Sequential(nn.Conv1d(c7, 8, kernel_size=7, padding=3, stride=2),\n",
    "                                            nn.BatchNorm1d(8),\n",
    "                                            nn.ReLU(inplace=True))\n",
    "        self.branch_pool = nn.Sequential(nn.Conv1d(in_channels, 8, kernel_size=1, stride=2),\n",
    "                                         nn.BatchNorm1d(8),\n",
    "                                         nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch5 = self.branch5_1(x)\n",
    "        branch5 = self.branch5_2(branch5)\n",
    "        branch3dbl = self.branch3dbl_1(x)\n",
    "        branch3dbl = self.branch3dbl_2(branch3dbl)\n",
    "        branch3dbl = self.branch3dbl_3(branch3dbl)\n",
    "\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = functional.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5, branch3dbl, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class AttentionModule1d_four_stage_1d(nn.Module):\n",
    "    # input size is 1*1024\n",
    "    def __init__(self, in_channels, out_channels, size1=256, size2=128, size3=64, size4=32):\n",
    "        super(AttentionModule1d_four_stage_1d, self).__init__()\n",
    "        self.size1 = size1\n",
    "        self.size2 = size2\n",
    "        self.size3 = size3\n",
    "        self.size4 = size4\n",
    "        self.first_residual_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock_1d(in_channels, out_channels),\n",
    "            ResidualBlock_1d(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        # 512\n",
    "        self.softmax1_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "        self.skip1_connection_residual_block = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "        self.mpool2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        # 258\n",
    "        self.softmax2_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "        self.skip2_connection_residual_block = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "        self.mpool3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        # 128\n",
    "        self.softmax3_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "        self.skip3_connection_residual_block = ResidualBlock_1d(in_channels, out_channels)\n",
    "        self.mpool4 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        # 64\n",
    "        self.softmax4_blocks = nn.Sequential(\n",
    "            ResidualBlock_1d(in_channels, out_channels),\n",
    "            ResidualBlock_1d(in_channels, out_channels)\n",
    "        )\n",
    "        self.softmax5_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "        self.softmax6_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "        self.softmax7_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "        # nn.upsample\n",
    "        self.softmax8_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.last_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        #         print(\"out_trunk\", out_trunk.shape)\n",
    "\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        #         print(\"out_softmax1\", out_softmax1.shape)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "        out_mpool2 = self.mpool2(out_softmax1)\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "        #         print(\"out_softmax2\", out_softmax2.shape)\n",
    "\n",
    "        out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "        out_mpool3 = self.mpool3(out_softmax2)\n",
    "        out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
    "        #         print(\"out_softmax3\", out_softmax3.shape)\n",
    "\n",
    "        out_skip3_connection = self.skip3_connection_residual_block(out_softmax3)\n",
    "        out_mpool4 = self.mpool4(out_softmax3)\n",
    "        out_softmax4 = self.softmax4_blocks(out_mpool4)\n",
    "        #         print(\"out_softmax4\", out_softmax4.shape)#         64, 256, 16\n",
    "\n",
    "        out_interp4 = nn.functional.interpolate(input=out_softmax4, size=self.size4) + out_softmax3\n",
    "\n",
    "        out = out_interp4 + out_skip3_connection\n",
    "        out_softmax5 = self.softmax5_blocks(out)\n",
    "        out_interp3 = nn.functional.interpolate(input=out_softmax5, size=self.size3) + out_softmax2\n",
    "\n",
    "        out = out_interp3 + out_skip2_connection\n",
    "\n",
    "        out_softmax6 = self.softmax6_blocks(out)\n",
    "\n",
    "        out_interp2 = nn.functional.interpolate(input=out_softmax6, size=self.size2) + out_softmax1\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        out_softmax7 = self.softmax7_blocks(out)\n",
    "        out_interp1 = nn.functional.interpolate(input=out_softmax7, size=self.size1) + out_trunk\n",
    "        out_softmax8 = self.softmax8_blocks(out_interp1)\n",
    "        out = (1 + out_softmax8) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        out_last = functional.dropout(out_last, p=0.2, training=self.training)\n",
    "        return out_last\n",
    "\n",
    "\n",
    "class ResidualAttentionModel_1d_two_attention_with_inceptionC_improve(nn.Module):\n",
    "    # for input size 1024\n",
    "    def __init__(self, classnumber):\n",
    "        super(ResidualAttentionModel_1d_two_attention_with_inceptionC_improve, self).__init__()\n",
    "        self.InceptionC_1d = InceptionC_1d_improve(in_channels=1)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.mpool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(58, 16, 3)\n",
    "\n",
    "        self.attention_module1 = AttentionModule1d_four_stage_1d(16, 16)\n",
    "\n",
    "        self.layer2 = self._make_layer(16, 32, 3)\n",
    "        self.attention_module2 = AttentionModule1d_four_stage_1d(32, 32)\n",
    "\n",
    "        self.layer3 = self._make_layer(32, 64, 3, stride=2)\n",
    "\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=7, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(7808, classnumber)\n",
    "\n",
    "    def _make_layer(self, inchannel, outchannel, block_num, stride=1):\n",
    "        \"\"\"\n",
    "        构建layer,包含多个residual block\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock_1d(inchannel, outchannel, stride))\n",
    "        #         at least one\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock_1d(outchannel, outchannel))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_inception = self.InceptionC_1d(x)\n",
    "        out = self.conv1(x)\n",
    "        out = torch.cat([x_inception, out], 1)\n",
    "        out = self.mpool1(out)\n",
    "        out = functional.dropout(out, p=0.1, training=self.training)\n",
    "        out = self.layer1(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.attention_module1(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.layer2(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.attention_module2(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.layer3(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.mpool2(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# ---------------\n",
    "# 对照测试网络\n",
    "class AttentionModule1d_no_stage_1d(nn.Module):\n",
    "    # input size is 1*1024\n",
    "    def __init__(self, in_channels, out_channels, size1=256, size2=128, size3=64, size4=32):\n",
    "        super(AttentionModule1d_no_stage_1d, self).__init__()\n",
    "        self.size1 = size1\n",
    "        self.size2 = size2\n",
    "        self.size3 = size3\n",
    "        self.size4 = size4\n",
    "\n",
    "        self.first_residual_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock_1d(in_channels, out_channels),\n",
    "            ResidualBlock_1d(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        # 512\n",
    "        self.softmax1_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "        self.softmax8_blocks = nn.Sequential(\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.last_blocks = ResidualBlock_1d(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        #         print(\"out_trunk\", out_trunk.shape)\n",
    "\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        #         print(\"out_softmax1\", out_softmax1.shape)\n",
    "\n",
    "        out_interp1 = nn.functional.interpolate(input=out_softmax1, size=self.size1) + out_trunk\n",
    "        out_softmax8 = self.softmax8_blocks(out_interp1)\n",
    "        out = (1 + out_softmax8) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "\n",
    "class ResidualAttentionModel_1d_two_attention_with_inceptionC_improve_duizhao(nn.Module):\n",
    "    # for input size 1024\n",
    "    # 对照网络, No stage\n",
    "    def __init__(self, classnumber):\n",
    "        super(ResidualAttentionModel_1d_two_attention_with_inceptionC_improve_duizhao, self).__init__()\n",
    "        self.InceptionC_1d = InceptionC_1d_improve(in_channels=1)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 8, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.mpool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(58, 16, 3)\n",
    "\n",
    "        self.attention_module1 = AttentionModule1d_no_stage_1d(16, 16)\n",
    "\n",
    "        self.layer2 = self._make_layer(16, 32, 3)\n",
    "        self.attention_module2 = AttentionModule1d_no_stage_1d(32, 32)\n",
    "\n",
    "        self.layer3 = self._make_layer(32, 64, 3, stride=2)\n",
    "\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=7, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(7808, classnumber)\n",
    "\n",
    "    def _make_layer(self, inchannel, outchannel, block_num, stride=1):\n",
    "        \"\"\"\n",
    "        构建layer,包含多个residual block\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock_1d(inchannel, outchannel, stride))\n",
    "        #         at least one\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock_1d(outchannel, outchannel))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_inception = self.InceptionC_1d(x)\n",
    "        out = self.conv1(x)\n",
    "        out = torch.cat([x_inception, out], 1)\n",
    "        out = self.mpool1(out)\n",
    "        out = functional.dropout(out, p=0.1, training=self.training)\n",
    "        out = self.layer1(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.attention_module1(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.layer2(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.attention_module2(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.layer3(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.mpool2(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 对照网络1\n",
    "class LuResNet_1D(nn.Module):\n",
    "    def __init__(self, classnumber):\n",
    "        super(LuResNet_1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 8, kernel_size=9, padding=4)\n",
    "        self.shortcut1 = nn.Conv1d(1, 8, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.mpool1 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.shortcut2 = nn.Conv1d(8, 16, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(8, 16, kernel_size=9, padding=4)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.mpool2 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.conv3 = nn.Conv1d(16, 32, kernel_size=9, padding=4)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.mpool3 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        self.dense = nn.Linear(512, classnumber)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_short1 = self.shortcut1(x)\n",
    "        out = self.conv1(x)\n",
    "        out = out + x_short1\n",
    "        #         N, 8, 1024\n",
    "        out = functional.leaky_relu(self.bn1(out))\n",
    "        out = self.mpool1(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        #         10, 8, 256\n",
    "        x_short2 = self.shortcut2(out)\n",
    "        out = self.conv2(out)\n",
    "        out = out + x_short2\n",
    "        out = functional.leaky_relu(self.bn2(out))\n",
    "        out = self.mpool2(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = self.conv3(out)\n",
    "        out = functional.leaky_relu(self.bn3(out))\n",
    "        out = self.mpool3(out)\n",
    "        out = functional.dropout(out, p=0.3, training=self.training)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AcharyaNet(nn.Module):\n",
    "    # Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals\n",
    "    def __init__(self, classnumber):\n",
    "        super(AcharyaNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 4, kernel_size=6, stride=1)\n",
    "        self.mpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv1d(4, 4, kernel_size=5, stride=1)\n",
    "        self.mpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv1d(4, 10, kernel_size=4, stride=1)\n",
    "        self.mpool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv1d(10, 10, kernel_size=4, stride=1)\n",
    "        self.mpool4 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv1d(10, 15, kernel_size=4, stride=1)\n",
    "        self.mpool5 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.dense1 = nn.Linear(280, 50)\n",
    "        self.dense2 = nn.Linear(50, 20)\n",
    "        self.dense3 = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.mpool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.mpool2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.mpool3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.mpool4(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.mpool4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dense1(out)\n",
    "        out = self.dense2(out)\n",
    "        out = self.dense3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ResidualAttentionModel_1d_two_attention_with_inceptionC_improve(classnumber=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/onnx/utils.py:586: UserWarning: ONNX export failed on ATen operator upsample_nearest1d because torch.onnx.symbolic.upsample_nearest1d does not exist\n",
      "  .format(op_name, op_name))\n"
     ]
    }
   ],
   "source": [
    "t = tw.draw_model(m, [1, 1, 1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(\"ResidualAttentionModel_1d_two_attention_with_inceptionC_improve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
