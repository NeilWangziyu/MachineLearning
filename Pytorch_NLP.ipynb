{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_data = [1,2,3]\n",
    "V = torch.Tensor(V_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "M_Data = [[1,2,3], [4,5,6]]\n",
    "M = torch.Tensor(M_Data)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_data = [[[1,2], [3,4]],[[5,6], [7,8]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "T = torch.Tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.2430, -0.0071,  1.3282, -1.3933,  0.4622],\n",
      "         [-0.2764, -1.2210,  0.8431,  1.0022,  0.1400],\n",
      "         [-1.1555,  0.9144, -0.6386, -0.2809, -2.0007],\n",
      "         [-0.4383, -1.9135,  1.5961, -0.0737,  0.1087]],\n",
      "\n",
      "        [[ 0.5461, -0.4756, -0.4275,  1.6071, -0.5357],\n",
      "         [-0.1147,  0.6570, -1.0857, -1.2134,  1.5178],\n",
      "         [ 1.0895,  1.1816,  1.8049,  0.5686, -0.1804],\n",
      "         [-0.1040, -0.1954, -0.1030, -0.7803, -0.6796]],\n",
      "\n",
      "        [[ 1.3413,  1.9875, -0.1457,  0.2885,  0.2464],\n",
      "         [ 0.1643, -0.5070, -0.5306,  0.4973, -1.0910],\n",
      "         [ 0.1839, -0.6231,  0.5895,  0.6499, -0.7294],\n",
      "         [-0.7590, -1.0900,  1.9198,  0.6902, -1.1122]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3,4,5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11ae4fc50>\n"
     ]
    }
   ],
   "source": [
    "x = autograd.Variable(torch.Tensor([1,2,3]), requires_grad = True)\n",
    "y = autograd.Variable(torch.Tensor([4,5,6]), requires_grad = True)\n",
    "z = x + y\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11ae4f0b8>\n",
      "tensor([[ 1.7441,  2.1293],\n",
      "        [-1.1714,  2.0727]])\n"
     ]
    }
   ],
   "source": [
    "var_x = torch.randn((2,2),requires_grad = True)\n",
    "var_x = torch.randn((2,2),requires_grad = True)\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn)\n",
    "var_z_data = var_z.data\n",
    "print(var_z.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9869, -0.5094, -0.0087],\n",
      "        [ 0.5162,  0.8296, -0.7762]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 3)\n",
    "data = torch.randn(2, 5, requires_grad = True)\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4593,  0.8147],\n",
      "        [ 1.1913, -2.4868]], requires_grad=True)\n",
      "tensor([[0.0000, 0.8147],\n",
      "        [1.1913, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2, 2, requires_grad = True)\n",
    "print(data)\n",
    "print(F.relu(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5426,  0.5777, -1.2939, -0.3694,  1.4929], requires_grad=True)\n",
      "tensor([0.1929, 0.1998, 0.0307, 0.0775, 0.4990], grad_fn=<SoftmaxBackward>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n",
      "tensor([-1.6455, -1.6104, -3.4819, -2.5574, -0.6951],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  after removing the cwd from sys.path.\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(5, requires_grad=True)\n",
    "print(data)\n",
    "print(F.softmax(data))\n",
    "print(F.softmax(data).sum())\n",
    "print(F.log_softmax(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('me gusta comer en la cafeteria'.split(), \"SPANISH\"), \n",
    "       ('give it to me'.split(), \"ENGLISH\"), \n",
    "       ('No cre0 que sea una buena idea'.split(),  \"SPANISH\"),\n",
    "       ('No it is not a food idea to get lost at sea'.split(), 'ENGLISH')]\n",
    "test_data = [('yo creo que si'.split(), \"SPANISH\"), \n",
    "            ('it is lost on me'.split(), \"ENGLISH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'food': 19, 'a': 18, 'give': 6, 'si': 25, 'que': 11, 'en': 3, 'No': 9, 'la': 4, 'una': 13, 'sea': 12, 'is': 16, 'idea': 15, 'me': 0, 'it': 7, 'at': 22, 'creo': 24, 'cre0': 10, 'not': 17, 'gusta': 1, 'buena': 14, 'comer': 2, 'on': 26, 'cafeteria': 5, 'yo': 23, 'to': 8, 'lost': 21, 'get': 20}\n"
     ]
    }
   ],
   "source": [
    "WORD_TO_IX = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in WORD_TO_IX:\n",
    "            WORD_TO_IX[word]= len(WORD_TO_IX)\n",
    "print(WORD_TO_IX)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(WORD_TO_IX)\n",
    "NUM_BABEL = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec))\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1502,  0.0295, -0.0327,  0.1416, -0.0519,  0.0789, -0.1835,  0.0651,\n",
      "         -0.1791, -0.1154, -0.0140, -0.0544,  0.0622,  0.0272,  0.0845, -0.1395,\n",
      "          0.0005,  0.1077,  0.1912,  0.1428, -0.0058,  0.0038, -0.0926, -0.0165,\n",
      "          0.0803,  0.1407,  0.0045],\n",
      "        [-0.1011, -0.1380, -0.0752,  0.1185, -0.1134,  0.1280, -0.0991, -0.0676,\n",
      "         -0.1642, -0.0101, -0.1823,  0.1239, -0.0853,  0.1793,  0.1206,  0.0864,\n",
      "         -0.1885, -0.0652, -0.1353, -0.0986, -0.0971, -0.1765,  0.0415,  0.0471,\n",
      "         -0.0818,  0.1187, -0.0441]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0365, -0.0277], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(NUM_BABEL, VOCAB_SIZE)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4786, -0.9666]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], WORD_TO_IX)\n",
    "log_probs = model(bow_vector)\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {'SPANISH':0, 'ENGLISH':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7270, -0.6604]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[-0.3733, -1.1662]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([ 0.0803, -0.0818], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for instance, lebel in test_data:\n",
    "    bow_vec = make_bow_vector(instance, word_to_ix=WORD_TO_IX)\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "print(next(model.parameters())[:,WORD_TO_IX[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for instance, label in data:\n",
    "        model.zero_grad()\n",
    "        bow_vec = make_bow_vector(instance, WORD_TO_IX)\n",
    "        target = make_target(label, label_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0537, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 1., 1., 1., 0.]])\n",
      "tensor([[-0.4690, -0.9825]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 1.]])\n",
      "tensor([[-1.2923, -0.3211]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for instance, label in test_data:\n",
    "    bow_vec = make_bow_vector(instance, WORD_TO_IX)\n",
    "    print(bow_vec)\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {\"hello\":0, \"world\":1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1127, -0.8782,  1.7724, -0.0710,  0.4250]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "embeds = nn.Embedding(2, 5)\n",
    "# 2 words in vocab, 5 dimensional embedding\n",
    "lookup_tensor = torch.LongTensor([word_to_ix['hello']])\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['When', 'forty'], 'winters'),\n",
       " (['forty', 'winters'], 'shall'),\n",
       " (['winters', 'shall'], 'besiege'),\n",
       " (['shall', 'besiege'], 'thy'),\n",
       " (['besiege', 'thy'], 'brow,'),\n",
       " (['thy', 'brow,'], 'And'),\n",
       " (['brow,', 'And'], 'dig'),\n",
       " (['And', 'dig'], 'deep'),\n",
       " (['dig', 'deep'], 'trenches'),\n",
       " (['deep', 'trenches'], 'in'),\n",
       " (['trenches', 'in'], 'thy'),\n",
       " (['in', 'thy'], \"beauty's\"),\n",
       " (['thy', \"beauty's\"], 'field,'),\n",
       " ([\"beauty's\", 'field,'], 'Thy'),\n",
       " (['field,', 'Thy'], \"youth's\"),\n",
       " (['Thy', \"youth's\"], 'proud'),\n",
       " ([\"youth's\", 'proud'], 'livery'),\n",
       " (['proud', 'livery'], 'so'),\n",
       " (['livery', 'so'], 'gazed'),\n",
       " (['so', 'gazed'], 'on'),\n",
       " (['gazed', 'on'], 'now,'),\n",
       " (['on', 'now,'], 'Will'),\n",
       " (['now,', 'Will'], 'be'),\n",
       " (['Will', 'be'], 'a'),\n",
       " (['be', 'a'], \"totter'd\"),\n",
       " (['a', \"totter'd\"], 'weed'),\n",
       " ([\"totter'd\", 'weed'], 'of'),\n",
       " (['weed', 'of'], 'small'),\n",
       " (['of', 'small'], 'worth'),\n",
       " (['small', 'worth'], 'held:'),\n",
       " (['worth', 'held:'], 'Then'),\n",
       " (['held:', 'Then'], 'being'),\n",
       " (['Then', 'being'], 'asked,'),\n",
       " (['being', 'asked,'], 'where'),\n",
       " (['asked,', 'where'], 'all'),\n",
       " (['where', 'all'], 'thy'),\n",
       " (['all', 'thy'], 'beauty'),\n",
       " (['thy', 'beauty'], 'lies,'),\n",
       " (['beauty', 'lies,'], 'Where'),\n",
       " (['lies,', 'Where'], 'all'),\n",
       " (['Where', 'all'], 'the'),\n",
       " (['all', 'the'], 'treasure'),\n",
       " (['the', 'treasure'], 'of'),\n",
       " (['treasure', 'of'], 'thy'),\n",
       " (['of', 'thy'], 'lusty'),\n",
       " (['thy', 'lusty'], 'days;'),\n",
       " (['lusty', 'days;'], 'To'),\n",
       " (['days;', 'To'], 'say,'),\n",
       " (['To', 'say,'], 'within'),\n",
       " (['say,', 'within'], 'thine'),\n",
       " (['within', 'thine'], 'own'),\n",
       " (['thine', 'own'], 'deep'),\n",
       " (['own', 'deep'], 'sunken'),\n",
       " (['deep', 'sunken'], 'eyes,'),\n",
       " (['sunken', 'eyes,'], 'Were'),\n",
       " (['eyes,', 'Were'], 'an'),\n",
       " (['Were', 'an'], 'all-eating'),\n",
       " (['an', 'all-eating'], 'shame,'),\n",
       " (['all-eating', 'shame,'], 'and'),\n",
       " (['shame,', 'and'], 'thriftless'),\n",
       " (['and', 'thriftless'], 'praise.'),\n",
       " (['thriftless', 'praise.'], 'How'),\n",
       " (['praise.', 'How'], 'much'),\n",
       " (['How', 'much'], 'more'),\n",
       " (['much', 'more'], 'praise'),\n",
       " (['more', 'praise'], \"deserv'd\"),\n",
       " (['praise', \"deserv'd\"], 'thy'),\n",
       " ([\"deserv'd\", 'thy'], \"beauty's\"),\n",
       " (['thy', \"beauty's\"], 'use,'),\n",
       " ([\"beauty's\", 'use,'], 'If'),\n",
       " (['use,', 'If'], 'thou'),\n",
       " (['If', 'thou'], 'couldst'),\n",
       " (['thou', 'couldst'], 'answer'),\n",
       " (['couldst', 'answer'], \"'This\"),\n",
       " (['answer', \"'This\"], 'fair'),\n",
       " ([\"'This\", 'fair'], 'child'),\n",
       " (['fair', 'child'], 'of'),\n",
       " (['child', 'of'], 'mine'),\n",
       " (['of', 'mine'], 'Shall'),\n",
       " (['mine', 'Shall'], 'sum'),\n",
       " (['Shall', 'sum'], 'my'),\n",
       " (['sum', 'my'], 'count,'),\n",
       " (['my', 'count,'], 'and'),\n",
       " (['count,', 'and'], 'make'),\n",
       " (['and', 'make'], 'my'),\n",
       " (['make', 'my'], 'old'),\n",
       " (['my', 'old'], \"excuse,'\"),\n",
       " (['old', \"excuse,'\"], 'Proving'),\n",
       " ([\"excuse,'\", 'Proving'], 'his'),\n",
       " (['Proving', 'his'], 'beauty'),\n",
       " (['his', 'beauty'], 'by'),\n",
       " (['beauty', 'by'], 'succession'),\n",
       " (['by', 'succession'], 'thine!'),\n",
       " (['succession', 'thine!'], 'This'),\n",
       " (['thine!', 'This'], 'were'),\n",
       " (['This', 'were'], 'to'),\n",
       " (['were', 'to'], 'be'),\n",
       " (['to', 'be'], 'new'),\n",
       " (['be', 'new'], 'made'),\n",
       " (['new', 'made'], 'when'),\n",
       " (['made', 'when'], 'thou'),\n",
       " (['when', 'thou'], 'art'),\n",
       " (['thou', 'art'], 'old,'),\n",
       " (['art', 'old,'], 'And'),\n",
       " (['old,', 'And'], 'see'),\n",
       " (['And', 'see'], 'thy'),\n",
       " (['see', 'thy'], 'blood'),\n",
       " (['thy', 'blood'], 'warm'),\n",
       " (['blood', 'warm'], 'when'),\n",
       " (['warm', 'when'], 'thou'),\n",
       " (['when', 'thou'], \"feel'st\"),\n",
       " (['thou', \"feel'st\"], 'it'),\n",
       " ([\"feel'st\", 'it'], 'cold.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word : i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, congext_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(1, -1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view(1, -1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[30, 33]\n",
      "[33, 55]\n",
      "[55, 11]\n",
      "[11, 40]\n",
      "[40, 2]\n",
      "[2, 50]\n",
      "[50, 89]\n",
      "[89, 23]\n",
      "[23, 12]\n",
      "[12, 10]\n",
      "[10, 31]\n",
      "[31, 2]\n",
      "[2, 21]\n",
      "[21, 94]\n",
      "[94, 35]\n",
      "[35, 95]\n",
      "[95, 4]\n",
      "[4, 3]\n",
      "[3, 65]\n",
      "[65, 47]\n",
      "[47, 91]\n",
      "[91, 18]\n",
      "[18, 7]\n",
      "[7, 48]\n",
      "[48, 76]\n",
      "[76, 84]\n",
      "[84, 44]\n",
      "[44, 52]\n",
      "[52, 96]\n",
      "[96, 36]\n",
      "[36, 74]\n",
      "[74, 26]\n",
      "[26, 29]\n",
      "[29, 90]\n",
      "[90, 32]\n",
      "[32, 0]\n",
      "[0, 2]\n",
      "[2, 63]\n",
      "[63, 51]\n",
      "[51, 27]\n",
      "[27, 0]\n",
      "[0, 93]\n",
      "[93, 85]\n",
      "[85, 52]\n",
      "[52, 2]\n",
      "[2, 75]\n",
      "[75, 56]\n",
      "[56, 77]\n",
      "[77, 19]\n",
      "[19, 42]\n",
      "[42, 13]\n",
      "[13, 86]\n",
      "[86, 12]\n",
      "[12, 22]\n",
      "[22, 6]\n",
      "[6, 41]\n",
      "[41, 59]\n",
      "[59, 67]\n",
      "[67, 54]\n",
      "[54, 92]\n",
      "[92, 34]\n",
      "[34, 24]\n",
      "[24, 28]\n",
      "[28, 82]\n",
      "[82, 8]\n",
      "[8, 20]\n",
      "[20, 73]\n",
      "[73, 2]\n",
      "[2, 21]\n",
      "[21, 61]\n",
      "[61, 1]\n",
      "[1, 38]\n",
      "[38, 80]\n",
      "[80, 43]\n",
      "[43, 16]\n",
      "[16, 78]\n",
      "[78, 71]\n",
      "[71, 52]\n",
      "[52, 83]\n",
      "[83, 15]\n",
      "[15, 39]\n",
      "[39, 81]\n",
      "[81, 45]\n",
      "[45, 92]\n",
      "[92, 66]\n",
      "[66, 81]\n",
      "[81, 53]\n",
      "[53, 49]\n",
      "[49, 5]\n",
      "[5, 14]\n",
      "[14, 63]\n",
      "[63, 60]\n",
      "[60, 64]\n",
      "[64, 46]\n",
      "[46, 72]\n",
      "[72, 25]\n",
      "[25, 87]\n",
      "[87, 48]\n",
      "[48, 58]\n",
      "[58, 17]\n",
      "[17, 69]\n",
      "[69, 38]\n",
      "[38, 70]\n",
      "[70, 57]\n",
      "[57, 89]\n",
      "[89, 9]\n",
      "[9, 2]\n",
      "[2, 68]\n",
      "[68, 37]\n",
      "[37, 69]\n",
      "[69, 38]\n",
      "[38, 62]\n",
      "[62, 88]\n",
      "[tensor([523.3656]), tensor([521.0344]), tensor([518.7178]), tensor([516.4152]), tensor([514.1273]), tensor([511.8524]), tensor([509.5903]), tensor([507.3394]), tensor([505.1010]), tensor([502.8730]), tensor([500.6558]), tensor([498.4468]), tensor([496.2462]), tensor([494.0533]), tensor([491.8671]), tensor([489.6870]), tensor([487.5131]), tensor([485.3464]), tensor([483.1853]), tensor([481.0276])]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    for context, target in triframs:\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables\n",
    "        context_idxs = list(map(lambda w:word_to_ix[w], context))\n",
    "        print(context_idxs)\n",
    "        context_var = torch.LongTensor(context_idxs)\n",
    "        \n",
    "        # Step 2. Recall that torch *accumulates* gradients.  Before passing in a new instance,\n",
    "        # you need to zero out the gradients from the old instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 3. Run the forward pass, getting log probabilities over next words\n",
    "        log_prob = model(context_var)\n",
    "        loss = loss_function(log_prob, torch.LongTensor([word_to_ix[target]]))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data\n",
    "    \n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONTEXT_SIZE = 2 # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process. Computational processes are abstract\n",
    "beings that inhabit computers. As they evolve, processes manipulate other abstract\n",
    "things called data. The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "word_to_ix = { word: i for i, word in enumerate(set(raw_text)) }\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [ raw_text[i-2], raw_text[i-1], raw_text[i+1], raw_text[i+2] ]\n",
    "    target = raw_text[i]\n",
    "    data.append( (context, target) )\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 26, 30, 42]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 26, 30, 42])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create your model and train.  here are some functions to help you make the data ready for use by your module\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = list(map(lambda w: word_to_ix[w], context))\n",
    "    print(idxs)\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "make_context_vector(data[0][0], word_to_ix) # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1.5634, 0.9127, 0.6512]]),\n",
       " tensor([[-0.1519,  0.6428, -0.1296]]),\n",
       " tensor([[-0.2298,  0.5477,  0.3164]]),\n",
       " tensor([[0.7692, 0.6142, 2.1560]]),\n",
       " tensor([[-0.2368,  1.2858, -0.1171]])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.5431,  0.0763, -1.2461]]]), tensor([[[0.4701, 1.1985, 0.6401]]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = (torch.randn(1,1,3), torch.randn(1,1,3))\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.7198,  0.5704, -1.1909]]), tensor([[-2.3201,  1.5620,  0.2520]]), tensor([[ 0.4881, -0.1031,  0.6183]]), tensor([[ 0.6253, -1.3380, -1.4148]]), tensor([[-0.5018, -1.2962,  0.7409]])]\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(3, 3)\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]\n",
    "\n",
    "hidden = (torch.randn(1,1,3), torch.randn(1,1,3))\n",
    "\n",
    "for i in inputs:\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "print(inputs)\n",
    "\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "hidden = (torch.randn(1,1,3), torch.randn(1,1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7615, -0.4533,  0.5973]]]),\n",
       " tensor([[[-0.2464, -0.2153,  0.0371]]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out, hidden = lstm(inputs, hidden)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden = lstm(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0457, -0.1372,  0.1554]],\n",
       "\n",
       "        [[ 0.2805, -0.1643,  0.0381]],\n",
       "\n",
       "        [[ 0.2576,  0.0371,  0.1937]],\n",
       "\n",
       "        [[ 0.2866, -0.0131,  0.1137]],\n",
       "\n",
       "        [[ 0.4438,  0.0524,  0.1285]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.4438, 0.0524, 0.1285]]], grad_fn=<StackBackward>),\n",
       " tensor([[[0.8040, 0.1381, 0.3358]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w:to_ix[w], seq))\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "tag_to_ix = {'DET':0, 'NN':1, \"V\":2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1,1,self.hidden_dim))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores\n",
    "    \n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7822, -1.1816, -1.4448],\n",
       "        [-0.8182, -1.2010, -1.3552],\n",
       "        [-0.8409, -1.1557, -1.3710],\n",
       "        [-0.7655, -1.2714, -1.3687],\n",
       "        [-0.7772, -1.2004, -1.4304]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_score = model(inputs)\n",
    "tag_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'ate', 'the', 'apple']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300): # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        \n",
    "        # Step 1. Remember that Pytorch accumulates gradients.  We need to clear them out\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Also, we need to clear out the hidden state of the LSTM, detaching it from its\n",
    "        # history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "    \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into Variables\n",
    "        # of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "    \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5617, -0.8879, -4.0039],\n",
       "        [-3.5406, -0.0377, -4.8264],\n",
       "        [-3.5701, -4.1509, -0.0449],\n",
       "        [-0.0321, -3.8560, -4.5592],\n",
       "        [-4.3360, -0.0166, -5.6835]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "tag_score = model(inputs)\n",
    "tag_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['The', 'dog', 'ate', 'the', 'apple'], ['DET', 'NN', 'V', 'DET', 'NN']),\n",
       " (['Everybody', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN'])]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM Conditional Random Field for Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to make the code more readable.\n",
    "def to_scalar(var):\n",
    "    # returns a python float\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        \n",
    "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        \n",
    "        # These two statements enforce the constraint that we never transfer *to* the start tag,\n",
    "        # and we never transfer *from* the stop tag (the model would probably learn this anyway,\n",
    "        # so this enforcement is likely unimportant)\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim), torch.randn(2, 1, self.hidden_dim))\n",
    "    \n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        \n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = [] # The forward variables at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of the previous tag\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the edge (i -> next_tag)\n",
    "                # before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    " \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = autograd.Variable( torch.Tensor([0]) )\n",
    "        tags = torch.cat( [torch.LongTensor([self.tag_to_ix[START_TAG]]), tags] )\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.transitions[tags[i+1], tags[i]] + feat[tags[i+1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        \n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        \n",
    "        # forward_var at step i holds the viterbi variables for step i-1 \n",
    "        forward_var = autograd.Variable(init_vvars)\n",
    "        for feat in feats:\n",
    "            bptrs_t = [] # holds the backpointers for this step\n",
    "            viterbivars_t = [] # holds the viterbi variables for this step\n",
    "            \n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the previous step,\n",
    "                # plus the score of transitioning from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id])\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "        \n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "        \n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    " \n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        self.hidden = self.init_hidden()\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "        \n",
    "    def forward(self, sentence): # dont confuse this with _forward_alg above.\n",
    "        self.hidden = self.init_hidden()\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "        \n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Make up some training data\n",
    "training_data = [ (\n",
    "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "    \"B I I I O O O B I O O\".split()\n",
    "), (\n",
    "    \"georgia tech is a university in georgia\".split(),\n",
    "    \"B I O O O O B\".split()\n",
    ") ]\n",
    "\n",
    "word_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "tag_to_ix = { \"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-e17b5b217045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprecheck_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprecheck_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecheck_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-245-3af309e0966c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Find the best path, given the features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-245-3af309e0966c>\u001b[0m in \u001b[0;36m_viterbi_decode\u001b[0;34m(self, feats)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnext_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0;31m# next_tag_var[i] holds the viterbi variable for tag i at the previous step,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m# plus the score of transitioning from tag i to next_tag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# Check predictions before training\n",
    "precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "precheck_tags = torch.LongTensor([tag_to_ix[t] for t in training_data[0][1]])\n",
    "model(precheck_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precheck_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precheck_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tag_to_ix[t] for t in training_data[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-6bdfbdade9c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecheck_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-b58bb1da1a95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Find the best path, given the features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viterbi_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-b58bb1da1a95>\u001b[0m in \u001b[0;36m_viterbi_decode\u001b[0;34m(self, feats)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnext_tag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                 \u001b[0;31m# next_tag_var[i] holds the viterbi variable for tag i at the previous step,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m# plus the score of transitioning from tag i to next_tag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "model(precheck_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, {'<START>': 3, '<STOP>': 4, 'B': 0, 'I': 1, 'O': 2}, 5, 4)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
